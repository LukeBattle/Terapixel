---
title: "Terapixel Project"
author: "Michael Luke Battle"
date: "30/12/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
```

## Project Requirements

### Written Report Outline


What is the need for the project?

Justify your choice of response (i.e. the nature of, and your plan for, your project). To give strength to your argument you should reference to practice elsewhere (e.g. in academic literature, or industry practices)

Discuss and implementation of CRISP-DM

How successful has it been? Provide evidence, using appropriate evaluation methodologies, and comment on the strengths/weaknesses of your evidence in answering this question

What are the future implications for work in this area? If applicable, which areas of extension work are now possible due to the foundational work you have performed in this project?

A brief reflection on your personal and professional learning in undertaking this project. Here, you can comment on how you found the process, what you learned about the technologies and methodologies you used, which aspects you found most difficult/straightforward, and any conclusions which will inform the way you undertake similar projects in the future

### Structured Abstract

REMEMBER TO INCLUDE STRUCTURED ABSTRACT

### EDA Findings.

You should also produce additional documentation detailing the findings from your exploratory analysis. You are encouraged to make use of a literate programming framework, e.g. R Markdown, to align analytic code with narrative  tetx. You should submit the source file(s) for the notebooks as well as output saved in PDF Format. No limit on length for this document!

CRISP-DM Methodology:

## Business Understanding

### Determine Business Objectives

Background

Maybe include more background information on Newcastle University and data

With world-leading research in data, Newcastle University is always undertaking new challenges to advance the field. In order to keep up with ever-expanding amount of data produced by cities, one such challenge is to be able to effectively convey the information from this data to stakeholders. To address this issue, the University has created a terapixel image of the city of Newcastle upon Tyne, including environmental data such as temperature and humidity, from sensors across the city. This terapixel image contains over one trillion pixels and not only allows the entire city to be viewed, but is rendered at 12 different levels allowing users to zoom in whilst retaining full picture detail. However, rendering over one trillion pixels requires immense computing power, and so the University has made use of public cloud service *Microsoft Azure* to perform the rendering process. This facilitates a far quicker rendering process, which is more suitable for a primary requirement of the project - to ensure the terapixel image supports daily updates.

Business Objectives

Whilst Newcastle University has created this terapixel visualisation, it is important that the rendering process with cloud supercomputing is rigorously evaluated in order to identify any inefficiencies.  

Business Success Criteria

### Assess Situation

### Determine Data Mining Goals

### Produce Project Plan

## Data Understanding

```{r include=FALSE}
#load project
library(ProjectTemplate)
load.project()

```

```{r include=FALSE}
source("src/eda_plots.R")
```


Describe Data

In order to evaluate the rendering process, Newcastle University has provided three files detailing various aspects of a single run of the rendering process. The file "application.checkpoints.csv" has dimensions and variable names:

```{r}
dim(application.checkpoints)
colnames(application.checkpoints)
str(application.checkpoints)
```
Where the "eventType" can be either "START" or "STOP" to indicate the end of beginning of an event, and "eventName" can take the following values;

```{r}
unique(application.checkpoints$eventName)
```
In which "TotalRender" is a combination of all other events, which represent different parts of the whole rendering process. In addition to "eventType" and "eventName", "timestamp" gives the time associated with this event starting or stopping, "hostname" is the hostname of the virtual machine auto-assigned by the Azure batch system and "jobId" and "taskId" are the unique IDs of the Azure batch job and task, respectively.

The file "gpu.csv" has the following dimensions and variable names, with all variable values being of class character:

```{r}
dim(gpu)
colnames(gpu)
str(gpu)
```

The variable names "timestamp" and "hostname" have the same meaning as in "application.checkpoints.csv". "gpuSerial" and "gpuUUID" are the serial number and unique ID of the physical GPU card, which represent a one-to-one match the "hostname" variable. As there are 1024 gpu cores, there are 1024 different values for "hostname", "gpuSerial" and "gpuUUID". 

```{r}
length(unique(gpu$hostname))
length(unique(gpu$gpuSerial))
length(unique(gpu$gpuUUID))

dim(unique(gpu[c("hostname","gpuSerial","gpuUUID")]))
```

The additional variables in this data set are quantitative variables describing different aspects of the cores. These include "powerDrawWatt" (the power draw of the GPU in watts), "gpuTempC" (the temperature of the GPU in Celsius), "gpuUtilPerc" (the percent utilisation of the GPU core) and "gpuMemUtilPerc" (the percent utilisation of the GPU memory).

We can also observe that variables "timestamp", "hostname" and "gpuUUID" are of class character, whilst variables "gpuSerial" and "powerDrawWatt" are of class numeric. Lastly, variables "gpuTempC", "gpuUtilPerc" and "gpuMemUtilPerc" are integer values.

```{r}
str(gpu)
```

The final file that we will use is "task.x.y.csv". This file has the dimensions and variables:

```{r}
dim(task.x.y)
colnames(task.x.y)

```


Variables "jobId" and "taskId" are the same as in "application.checkpoints.csv", whilst "x" and "y" represent the location of each pixel being rendered. The "Level" variable allows users to zoom into the visualisation. In total there are 12 levels, however only levels 12, 8 and 4 and rendered whilst the other levels are derived in the tiling process. Level 12 is when the the image at maximum zoom, and level 1 is when the image is zoomed right out. The "jobId" relates to the "Level" variable, so in total there are three different "jobId" values corresponding to the three levels that are rendered, as shown below:

```{r}
unique(task.x.y[c("jobId","level")])
```
We can also observe that the values in "taskId" and "jobId" are of class character, whilst the other variables are of class integer.

```{r}
str(task.x.y)
```


Data Quality

With the data described at a high level, it is necessary to assess the quality of it. First we can check for any missing values using the below code.

```{r}
sum(is.na(application.checkpoints))

sum(is.na(gpu))

sum(is.na(task.x.y))
```

As we are satisfied that there is no missing data, we can now check the data for duplicates.

```{r}
dim(application.checkpoints)[1] - dim(unique(application.checkpoints))[1]

dim(gpu)[1] - dim(unique(gpu))[1]

dim(task.x.y)[1] - dim(unique(task.x.y))[1]
```

We can see that there are 2,470 duplicates in "application.checkpoints.csv", 9 in "gpu.csv" and none in "task.x.y.csv". With no missing data and only a small percentage of the data in duplicate, the data is of good quality with respect to duplicates and missing data.

Although there is no missing data in the sense of NULL values, "gpu" has a significant population of outliers. We can observe this by plotting histograms for each quantitative variable in the data set. 

```{r echo=FALSE}
grid.arrange(gputemp_plot,gpupowerdraw_plot,gpuutil_plot,gpumemutil_plot,ncol = 2,nrow =2)
```

From the above histograms we can can observe significant noise in variables gpuUtilPerc, gpuMemUtilPerc and powerDrawWatt. Specifically, in gpuMemUtilPerc and gpuUtilPerc this noise is localised around 0. In the following section these outliers will be investigated and the data transformed into the a more suitable format for analysis.

## Data Preparation

In order to rigorously evaluate the supercomputer rendering process, it is important to examine all aspects of it. We will therefore use the data in all three files for our analysis. First of all, we will prepare the data in "application.checkpoints".

As the data in "application.checkpoints" is clean, we can proceed with using it to transform the data into a more useful format whilst deriving new attributes from it. To make the "timestamp" variable more meaningful, we can convert it from class character to datetime format, or POSIXct. As each value in "timestamp" is of the form "`r application.checkpoints$timestamp[1]`", we can use the below function to remove the letters "T" and "Z".

```{r eval = FALSE}
clean_date_time = function(string) {
  string_rep = str_replace(string, "T"," ")
  string_rep2 = str_replace(string_rep, "Z","")
  return(string_rep2)
}
```

Then, the resulting "timestamp" values are parsed into into POSIXct form, and the duplicates that were found in the *Data Understanding* section of this report are removed.

```{r eval = FALSE}

#apply clean_date_time function to date_time in data and create list
date_time_chr = lapply(application.checkpoints$timestamp,clean_date_time)

#create list of date-times converted to datetime format
date_time = parse_date_time(date_time_chr,"%Y%m%d %H%M%S")

#change timestamp to converted date_time format
app_data$timestamp = date_time

#remove duplicates
app_data = unique(app_data)

```

With the "timestamp" variable in a more computational-friendly format, we can now derive the runtime for each event. First, the data must be converted from long format to wide format with respect to the eventType variable. Then, the start time of the event can be subtracted from the stop time to calculate the runtime. This is performed with the below code.

```{r eval = FALSE}

#convert to wide format so that START and STOP are separate columns
app_wide = app_data %>%
  pivot_wider(names_from = eventType,
              values_from = timestamp)

#create runtime variable by calculating time difference from start to stop timestamp
app_wide$runtime = app_wide$STOP - app_wide$START

```

There are several issues with the data in "gpu" that we must address. Firstly, the values in the "timestamp" variable are in the same class and format as in the raw "application.checkpoints" file. As previously with "application.checkpoints", we take the same steps to transform this into a more meaningful and computationally compatible variable. We also remove the duplicates from the data. With the "timestamp" values now of class POSIXct and duplicates removed, we can turn out attention to the outliers in the data. Whether these outliers should be retained in the data or not is dependent upon if they are meaningful or an error. To explore this, we can first check if the observations with 0 for variable "gpuUtilPerc" and "gpuUtilPerc" are limited to a subset of GPUs or to all. If they are limited to a subset of them, this may express a different characteristic specific to those GPUs, in which case these outliers may have some meaning. We can check this with the below code that counts how many unique GPUs have instances where both "gpuUtilPerc" and "gpuMemUtilPerc" take a value of 0.

```{r}
length(unique(filter(gpu_data,gpuUtilPerc == 0 & gpuMemUtilPerc == 0)$gpuSerial))
```

As there is an observation where both variables have a value of 0 for each GPU in the supercomputer, it is clear that this does not tell us anything specific about the GPU. Another possible explanation of these is that they related to the pixel that is being rendered. However, a closer inspection of an instance where such an observation occurs shows that 

As the task.x.y data set contains no duplicates or missing values, there is no need for any cleaning of this data.

Integrate Data

With each data set reformatted, we can commence merging them to derive additional interpretations from the data. To begin, we can assign a TotalRender runtime to each pixel in the terapixel image by combining the data from "app_wide" and "task.x.y". First, we create a new data set containing only values with eventType "TotalRender". We can then confirm that the variable "taskId" is both a primary key in this data set as well as a foreign key for "task.x.y".

```{r}
totalrender_data %>%
  count(taskId) %>%
  filter(n > 1)

task.x.y %>%
  count(totalrender_data$taskId)

task.x.y %>%
  count(totalrender_data$taskId) %>%
  filter(n>1)
```

We can therefore join both data sets simply using the "taskId" variable with the below code:

```{r eval = FALSE}
app_task_data = left_join(totalrender_data,task_data[c("taskId","x","y","level")],by = c("taskId"))
```




Go through process of creating full merged dataset to combine all three

## Modelling

Create several graphs to view different interactions of data set

Which event types dominate task runtimes? (app checkpoints)

What is the interplay between GPU temperature and performance? (gpu)

What is the interplay between increased power draw and render time?
-- Can we quantify the variation in computation requirements for particular tiles?

Can we identify particular GPU cards (based on their serial numbers) whose performance differs to other cards? (i.e. perpetually slow cards).

What can we learn about the efficiency of the task scheduling process?

## Evaluation

## Deployment



